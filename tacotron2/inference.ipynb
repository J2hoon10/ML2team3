{"cells":[{"cell_type":"markdown","metadata":{"id":"2gk0O9KR76Lb"},"source":["## Tacotron 2 inference code \n","Edit the variables **checkpoint_path** and **text** to match yours and run the entire code to generate plots of mel outputs, alignments and audio synthesis from the generated mel-spectrogram using Griffin-Lim."]},{"cell_type":"markdown","metadata":{"id":"1KdM-3kV76Lf"},"source":["#### Import libraries and setup matplotlib"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KfBCKegENWCV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install jamo\n","!pip install unidecode\n","!pip install pillow\n","!pip install librosa\n","!pip install matplotlib"],"metadata":{"id":"BFvgcE6P-7d4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLkmPd-_76Lg"},"outputs":[],"source":["%cd /content/drive/MyDrive/머신러닝 Teamproj/tacotron2-master # 해당 코드 파일로 경로 지정정\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pylab as plt\n","\n","import IPython.display as ipd\n","\n","import sys\n","sys.path.append('waveglow/')\n","import numpy as np\n","import torch\n","\n","from hparams import defaults\n","from model import Tacotron2\n","from layers import TacotronSTFT, STFT\n","from audio_processing import griffin_lim\n","from train import load_model\n","from text import text_to_sequence\n","from denoiser import Denoiser"]},{"cell_type":"code","source":["def load_model(hparams):\n","    model = Tacotron2(hparams).cuda()\n","    if hparams.fp16_run:\n","        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n","\n","    if hparams.distributed_run:\n","        model = apply_gradient_allreduce(model)\n","\n","    return model"],"metadata":{"id":"RlbBG1StHBuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIw7IAm876Lh"},"outputs":[],"source":["def plot_data(data, figsize=(16, 4)):\n","    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n","    for i in range(len(data)):\n","        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n","                       interpolation='none')"]},{"cell_type":"code","source":["class Struct:\n","    def __init__(self, **entries):\n","        self.__dict__.update(entries)"],"metadata":{"id":"yYR0c-F-HdZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8R8FYkq76Li"},"source":["#### Setup hparams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4k-Rk_676Li"},"outputs":[],"source":["hparams = Struct(**defaults)\n","hparams.n_mel_channels=80\n","hparams.sampling_rate =22050"]},{"cell_type":"markdown","metadata":{"id":"A_nXJbO776Li"},"source":["#### Load model from checkpoint"]},{"cell_type":"markdown","source":[],"metadata":{"id":"HpirCjkZNAlv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"izUFODoK76Lj"},"outputs":[],"source":["tacotron_check= \"/content/drive/MyDrive/머신러닝 Teamproj/checkpoint_10000\"\n","model = load_model(hparams)\n","model.load_state_dict(torch.load(tacotron_check)['state_dict'])\n","model.cuda().eval()"]},{"cell_type":"markdown","metadata":{"id":"cNhjAjki76Lj"},"source":["#### Load WaveGlow for mel2audio synthesis and denoiser"]},{"cell_type":"code","source":["!git clone https://github.com/NVIDIA/waveglow.git\n","!cd waveglow\n","!git submodule init\n","!git submodule update"],"metadata":{"id":"RY7YZ9juuLiV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpcjiqNN76Lk"},"outputs":[],"source":["%cd /content/drive/MyDrive/머신러닝 Teamproj/tacotron2-master/waveglow\n","waveglow_path = '/content/drive/MyDrive/머신러닝 Teamproj/tacotron2-master/waveglow_256channels_universal_v5.pt'\n","waveglow = torch.load(waveglow_path)['model']\n","waveglow.cuda().eval()\n","for k in waveglow.convinv:\n","    k.float()\n","denoiser = Denoiser(waveglow)"]},{"cell_type":"markdown","metadata":{"id":"L7CB-KAK76Lk"},"source":["#### Prepare text input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGatYWeZ76Ll"},"outputs":[],"source":["text = \"I can do this all day.\"\n","sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :] # english model은 'english_cleaners', korean model은 'korean_cleaners'를 사용\n","sequence = torch.autograd.Variable(\n","    torch.from_numpy(sequence)).cuda().long()\n","print(sequence)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"IJv0KjIKsi9w"}},{"cell_type":"markdown","metadata":{"id":"ykKmSkWl76Ll"},"source":["#### Decode text input and plot results"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"aIksvZWi76Ll"},"outputs":[],"source":["mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n","plot_data((mel_outputs.float().data.cpu().numpy()[0],\n","           mel_outputs_postnet.float().data.cpu().numpy()[0],\n","           alignments.float().data.cpu().numpy()[0]))"]},{"cell_type":"markdown","metadata":{"id":"FmPmh6v-76Lm"},"source":["#### Synthesize audio from spectrogram using WaveGlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HNN6fRe76Lm"},"outputs":[],"source":["with torch.no_grad():\n","    audio = waveglow.infer(mel_outputs_postnet, sigma=0.666).cuda()\n","ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"]},{"cell_type":"markdown","metadata":{"id":"pkwBR-pF76Lm"},"source":["#### (Optional) Remove WaveGlow bias"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BE13z03E76Lm"},"outputs":[],"source":["audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n","ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate) "]},{"cell_type":"markdown","source":["####Show spectrogram(dB) of the wav data"],"metadata":{"id":"VoDWKJQP1NWi"}},{"cell_type":"code","source":["import numpy as np\n","import librosa, librosa.display \n","import matplotlib.pyplot as plt\n","\n","FIG_SIZE = (15,10)\n","\n","file = \"/content/drive/MyDrive/ML2team3/code/tacotron2/kss_temp/1_0000.wav\" #wav 데이터 파일 경로 지정\n","\n","# load audio file with Librosa\n","sig, sr = librosa.load(file, sr=22050)\n","\n","fft = np.fft.fft(sig)\n","\n","# 복소공간 값 절댓갑 취해서, magnitude 구하기\n","magnitude = np.abs(fft) \n","\n","# Frequency 값 만들기\n","f = np.linspace(0,sr,len(magnitude))\n","\n","# 푸리에 변환을 통과한 specturm은 대칭구조로 나와서 high frequency 부분 절반을 날려고 앞쪽 절반만 사용한다.\n","left_spectrum = magnitude[:int(len(magnitude)/2)]\n","left_f = f[:int(len(magnitude)/2)]\n","\n","# STFT -> spectrogram\n","hop_length = 512  # 전체 frame 수\n","n_fft = 2048  # frame 하나당 sample 수\n","\n","# calculate duration hop length and window in seconds\n","hop_length_duration = float(hop_length)/sr\n","n_fft_duration = float(n_fft)/sr\n","\n","# STFT\n","stft = librosa.stft(sig, n_fft=n_fft, hop_length=hop_length)\n","\n","# 복소공간 값 절댓값 취하기\n","magnitude = np.abs(stft)\n","\n","# magnitude > Decibels \n","log_spectrogram = librosa.amplitude_to_db(magnitude)\n","\n","# display spectrogram\n","plt.figure(figsize=FIG_SIZE)\n","librosa.display.specshow(log_spectrogram, sr=sr, hop_length=hop_length)\n","plt.xlabel(\"Time\")\n","plt.ylabel(\"Frequency\")\n","plt.colorbar(format=\"%+2.0f dB\")\n","plt.title(\"Spectrogram (dB)\")"],"metadata":{"id":"l93o8PCMvGHZ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}